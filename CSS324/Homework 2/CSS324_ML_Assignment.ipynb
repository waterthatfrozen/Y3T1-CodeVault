{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CSS324-ML-Assignment-6222780379-6222771634",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1co9QUId36Eo"
      },
      "source": [
        "# CSS324 Homework Assignment\n",
        "\n",
        "CIFAR10 is a small image classification dataset. Its objective is to classification an 32x32 color image into 10 classes.\n",
        "\n",
        "See https://www.cs.toronto.edu/~kriz/cifar.html and https://keras.io/api/datasets/cifar10/ for more details.\n",
        "\n",
        "### Members\n",
        "6222780379 Paphana Yiwsiw (Section 3)\n",
        "\n",
        "6222771634 Minsu Yun (Section 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zXtrgRH9oK5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMSl6MYp33eW"
      },
      "source": [
        "# Load CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwoz3JDK98Jy"
      },
      "source": [
        "# Plot a training example\n",
        "# x = x_train[19, :, :, :]\n",
        "# y = y_train[19][0]\n",
        "# plt.imshow(x)\n",
        "# print(y)\n",
        "# 0 -> airplane / 1 -> automobile / 2 -> bird / 3 -> cat / 4 -> deer\n",
        "# 5 -> dog / 6 -> frog / 7 -> horse / 8 -> ship / 9 -> truck"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFt4OqeP_Buc"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Construct a deep neural network containing three hidden layer to classify images in the CIFAR10 dataset. You can choose the numbers of hidden nodes in three layers, appropriate activation functions, regularizers. Use 20% of the training set to validate the model.\n",
        "\n",
        "After the training process, print the training, validation, and test accuracies, as well as plot the training loss and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpuD33bU__mt"
      },
      "source": [
        "# Your implementation for Question 1\n",
        "tf.random.set_seed(11)\n",
        "# Preprocess\n",
        "x_train2 = x_train / 255.0\n",
        "x_test2 = x_test / 255.0\n",
        "y_train2 = tf.keras.utils.to_categorical(y_train)\n",
        "y_test2 = tf.keras.utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIjamOv6qUcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560b4698-1835-4202-d561-9d93a92496fd"
      },
      "source": [
        "# Define a model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(32,32,3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# complie model\n",
        "model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# set a checkpoint callback & train model\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = '/tmp/checkpoint',save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)\n",
        "history = model.fit(x_train2,y_train2,epochs=200,batch_size=1024,shuffle=True,validation_split=0.2,callbacks=[model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 3072)              0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 3072)             12288     \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               393344    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 411,162\n",
            "Trainable params: 404,666\n",
            "Non-trainable params: 6,496\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "40/40 [==============================] - 3s 32ms/step - loss: 2.4113 - accuracy: 0.2042 - val_loss: 2.4106 - val_accuracy: 0.1783\n",
            "Epoch 2/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.1312 - accuracy: 0.2643 - val_loss: 2.0165 - val_accuracy: 0.2938\n",
            "Epoch 3/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 2.0313 - accuracy: 0.2892 - val_loss: 1.9281 - val_accuracy: 0.3496\n",
            "Epoch 4/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.9740 - accuracy: 0.3066 - val_loss: 1.8485 - val_accuracy: 0.3777\n",
            "Epoch 5/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.9211 - accuracy: 0.3227 - val_loss: 1.8088 - val_accuracy: 0.3839\n",
            "Epoch 6/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.8796 - accuracy: 0.3376 - val_loss: 1.7688 - val_accuracy: 0.3883\n",
            "Epoch 7/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8471 - accuracy: 0.3486 - val_loss: 1.7340 - val_accuracy: 0.4027\n",
            "Epoch 8/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8241 - accuracy: 0.3520 - val_loss: 1.7049 - val_accuracy: 0.4116\n",
            "Epoch 9/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.7998 - accuracy: 0.3649 - val_loss: 1.6714 - val_accuracy: 0.4220\n",
            "Epoch 10/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.7812 - accuracy: 0.3710 - val_loss: 1.6517 - val_accuracy: 0.4276\n",
            "Epoch 11/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.7641 - accuracy: 0.3777 - val_loss: 1.6196 - val_accuracy: 0.4409\n",
            "Epoch 12/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.7484 - accuracy: 0.3847 - val_loss: 1.6091 - val_accuracy: 0.4444\n",
            "Epoch 13/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.7324 - accuracy: 0.3915 - val_loss: 1.6003 - val_accuracy: 0.4496\n",
            "Epoch 14/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.7180 - accuracy: 0.3943 - val_loss: 1.5875 - val_accuracy: 0.4475\n",
            "Epoch 15/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.7016 - accuracy: 0.3974 - val_loss: 1.5738 - val_accuracy: 0.4540\n",
            "Epoch 16/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.6898 - accuracy: 0.4081 - val_loss: 1.5592 - val_accuracy: 0.4630\n",
            "Epoch 17/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.6751 - accuracy: 0.4129 - val_loss: 1.5523 - val_accuracy: 0.4619\n",
            "Epoch 18/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.6798 - accuracy: 0.4114 - val_loss: 1.5364 - val_accuracy: 0.4678\n",
            "Epoch 19/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.6601 - accuracy: 0.4225 - val_loss: 1.5360 - val_accuracy: 0.4690\n",
            "Epoch 20/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.6477 - accuracy: 0.4271 - val_loss: 1.5204 - val_accuracy: 0.4759\n",
            "Epoch 21/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.6363 - accuracy: 0.4306 - val_loss: 1.5187 - val_accuracy: 0.4773\n",
            "Epoch 22/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.6300 - accuracy: 0.4322 - val_loss: 1.5126 - val_accuracy: 0.4724\n",
            "Epoch 23/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.6262 - accuracy: 0.4340 - val_loss: 1.5070 - val_accuracy: 0.4813\n",
            "Epoch 24/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.6168 - accuracy: 0.4405 - val_loss: 1.4962 - val_accuracy: 0.4799\n",
            "Epoch 25/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.5997 - accuracy: 0.4439 - val_loss: 1.4969 - val_accuracy: 0.4854\n",
            "Epoch 26/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.5948 - accuracy: 0.4433 - val_loss: 1.4959 - val_accuracy: 0.4845\n",
            "Epoch 27/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.5994 - accuracy: 0.4456 - val_loss: 1.4945 - val_accuracy: 0.4853\n",
            "Epoch 28/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5805 - accuracy: 0.4491 - val_loss: 1.4782 - val_accuracy: 0.4929\n",
            "Epoch 29/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5785 - accuracy: 0.4548 - val_loss: 1.4795 - val_accuracy: 0.4918\n",
            "Epoch 30/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5696 - accuracy: 0.4606 - val_loss: 1.4713 - val_accuracy: 0.4963\n",
            "Epoch 31/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.5657 - accuracy: 0.4563 - val_loss: 1.4681 - val_accuracy: 0.4970\n",
            "Epoch 32/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5664 - accuracy: 0.4623 - val_loss: 1.4676 - val_accuracy: 0.4985\n",
            "Epoch 33/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.5588 - accuracy: 0.4590 - val_loss: 1.4667 - val_accuracy: 0.4924\n",
            "Epoch 34/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.5516 - accuracy: 0.4643 - val_loss: 1.4578 - val_accuracy: 0.5014\n",
            "Epoch 35/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.5426 - accuracy: 0.4682 - val_loss: 1.4521 - val_accuracy: 0.5028\n",
            "Epoch 36/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.5431 - accuracy: 0.4705 - val_loss: 1.4595 - val_accuracy: 0.5029\n",
            "Epoch 37/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5416 - accuracy: 0.4708 - val_loss: 1.4491 - val_accuracy: 0.5052\n",
            "Epoch 38/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5361 - accuracy: 0.4762 - val_loss: 1.4507 - val_accuracy: 0.5017\n",
            "Epoch 39/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.5312 - accuracy: 0.4765 - val_loss: 1.4491 - val_accuracy: 0.5026\n",
            "Epoch 40/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.5294 - accuracy: 0.4753 - val_loss: 1.4446 - val_accuracy: 0.5090\n",
            "Epoch 41/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.5244 - accuracy: 0.4754 - val_loss: 1.4420 - val_accuracy: 0.5113\n",
            "Epoch 42/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.5189 - accuracy: 0.4828 - val_loss: 1.4413 - val_accuracy: 0.5128\n",
            "Epoch 43/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5183 - accuracy: 0.4831 - val_loss: 1.4426 - val_accuracy: 0.5101\n",
            "Epoch 44/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.5103 - accuracy: 0.4832 - val_loss: 1.4479 - val_accuracy: 0.5049\n",
            "Epoch 45/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5103 - accuracy: 0.4884 - val_loss: 1.4418 - val_accuracy: 0.5105\n",
            "Epoch 46/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.5163 - accuracy: 0.4877 - val_loss: 1.4436 - val_accuracy: 0.5098\n",
            "Epoch 47/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.5087 - accuracy: 0.4873 - val_loss: 1.4374 - val_accuracy: 0.5118\n",
            "Epoch 48/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.5025 - accuracy: 0.4883 - val_loss: 1.4351 - val_accuracy: 0.5106\n",
            "Epoch 49/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4921 - accuracy: 0.4941 - val_loss: 1.4342 - val_accuracy: 0.5148\n",
            "Epoch 50/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4986 - accuracy: 0.4929 - val_loss: 1.4304 - val_accuracy: 0.5176\n",
            "Epoch 51/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4939 - accuracy: 0.4952 - val_loss: 1.4327 - val_accuracy: 0.5147\n",
            "Epoch 52/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4891 - accuracy: 0.4940 - val_loss: 1.4250 - val_accuracy: 0.5224\n",
            "Epoch 53/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4884 - accuracy: 0.4971 - val_loss: 1.4279 - val_accuracy: 0.5195\n",
            "Epoch 54/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4905 - accuracy: 0.4953 - val_loss: 1.4260 - val_accuracy: 0.5182\n",
            "Epoch 55/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4880 - accuracy: 0.4993 - val_loss: 1.4239 - val_accuracy: 0.5189\n",
            "Epoch 56/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.4794 - accuracy: 0.5039 - val_loss: 1.4275 - val_accuracy: 0.5225\n",
            "Epoch 57/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4743 - accuracy: 0.5019 - val_loss: 1.4206 - val_accuracy: 0.5219\n",
            "Epoch 58/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4845 - accuracy: 0.5003 - val_loss: 1.4211 - val_accuracy: 0.5232\n",
            "Epoch 59/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4719 - accuracy: 0.5055 - val_loss: 1.4266 - val_accuracy: 0.5196\n",
            "Epoch 60/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4753 - accuracy: 0.5015 - val_loss: 1.4269 - val_accuracy: 0.5197\n",
            "Epoch 61/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4756 - accuracy: 0.5048 - val_loss: 1.4238 - val_accuracy: 0.5184\n",
            "Epoch 62/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4734 - accuracy: 0.5060 - val_loss: 1.4283 - val_accuracy: 0.5141\n",
            "Epoch 63/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4736 - accuracy: 0.5056 - val_loss: 1.4254 - val_accuracy: 0.5206\n",
            "Epoch 64/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4641 - accuracy: 0.5073 - val_loss: 1.4232 - val_accuracy: 0.5201\n",
            "Epoch 65/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4606 - accuracy: 0.5081 - val_loss: 1.4218 - val_accuracy: 0.5223\n",
            "Epoch 66/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4550 - accuracy: 0.5118 - val_loss: 1.4253 - val_accuracy: 0.5217\n",
            "Epoch 67/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4568 - accuracy: 0.5092 - val_loss: 1.4174 - val_accuracy: 0.5222\n",
            "Epoch 68/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4597 - accuracy: 0.5105 - val_loss: 1.4178 - val_accuracy: 0.5200\n",
            "Epoch 69/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4618 - accuracy: 0.5095 - val_loss: 1.4228 - val_accuracy: 0.5226\n",
            "Epoch 70/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4526 - accuracy: 0.5149 - val_loss: 1.4246 - val_accuracy: 0.5215\n",
            "Epoch 71/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4450 - accuracy: 0.5187 - val_loss: 1.4205 - val_accuracy: 0.5263\n",
            "Epoch 72/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.4462 - accuracy: 0.5140 - val_loss: 1.4110 - val_accuracy: 0.5300\n",
            "Epoch 73/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4518 - accuracy: 0.5136 - val_loss: 1.4115 - val_accuracy: 0.5304\n",
            "Epoch 74/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4461 - accuracy: 0.5160 - val_loss: 1.4156 - val_accuracy: 0.5262\n",
            "Epoch 75/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4354 - accuracy: 0.5220 - val_loss: 1.4264 - val_accuracy: 0.5204\n",
            "Epoch 76/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4436 - accuracy: 0.5180 - val_loss: 1.4209 - val_accuracy: 0.5230\n",
            "Epoch 77/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4413 - accuracy: 0.5214 - val_loss: 1.4142 - val_accuracy: 0.5267\n",
            "Epoch 78/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4424 - accuracy: 0.5191 - val_loss: 1.4112 - val_accuracy: 0.5281\n",
            "Epoch 79/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4517 - accuracy: 0.5171 - val_loss: 1.4132 - val_accuracy: 0.5254\n",
            "Epoch 80/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4466 - accuracy: 0.5208 - val_loss: 1.4206 - val_accuracy: 0.5270\n",
            "Epoch 81/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4340 - accuracy: 0.5221 - val_loss: 1.4131 - val_accuracy: 0.5281\n",
            "Epoch 82/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4404 - accuracy: 0.5200 - val_loss: 1.4202 - val_accuracy: 0.5224\n",
            "Epoch 83/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4296 - accuracy: 0.5245 - val_loss: 1.4188 - val_accuracy: 0.5255\n",
            "Epoch 84/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4382 - accuracy: 0.5227 - val_loss: 1.4112 - val_accuracy: 0.5283\n",
            "Epoch 85/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4457 - accuracy: 0.5181 - val_loss: 1.4170 - val_accuracy: 0.5233\n",
            "Epoch 86/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4369 - accuracy: 0.5241 - val_loss: 1.4198 - val_accuracy: 0.5254\n",
            "Epoch 87/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4335 - accuracy: 0.5240 - val_loss: 1.4145 - val_accuracy: 0.5285\n",
            "Epoch 88/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4250 - accuracy: 0.5258 - val_loss: 1.4177 - val_accuracy: 0.5314\n",
            "Epoch 89/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4318 - accuracy: 0.5227 - val_loss: 1.4155 - val_accuracy: 0.5267\n",
            "Epoch 90/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4258 - accuracy: 0.5282 - val_loss: 1.4103 - val_accuracy: 0.5320\n",
            "Epoch 91/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.4343 - accuracy: 0.5234 - val_loss: 1.4183 - val_accuracy: 0.5268\n",
            "Epoch 92/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4295 - accuracy: 0.5265 - val_loss: 1.4147 - val_accuracy: 0.5270\n",
            "Epoch 93/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.4305 - accuracy: 0.5228 - val_loss: 1.4187 - val_accuracy: 0.5293\n",
            "Epoch 94/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4251 - accuracy: 0.5290 - val_loss: 1.4191 - val_accuracy: 0.5290\n",
            "Epoch 95/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4221 - accuracy: 0.5293 - val_loss: 1.4124 - val_accuracy: 0.5291\n",
            "Epoch 96/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4127 - accuracy: 0.5350 - val_loss: 1.4101 - val_accuracy: 0.5286\n",
            "Epoch 97/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4255 - accuracy: 0.5275 - val_loss: 1.4164 - val_accuracy: 0.5295\n",
            "Epoch 98/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4264 - accuracy: 0.5282 - val_loss: 1.4164 - val_accuracy: 0.5264\n",
            "Epoch 99/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4326 - accuracy: 0.5255 - val_loss: 1.4232 - val_accuracy: 0.5318\n",
            "Epoch 100/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4301 - accuracy: 0.5249 - val_loss: 1.4290 - val_accuracy: 0.5267\n",
            "Epoch 101/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4262 - accuracy: 0.5290 - val_loss: 1.4149 - val_accuracy: 0.5267\n",
            "Epoch 102/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4195 - accuracy: 0.5314 - val_loss: 1.4131 - val_accuracy: 0.5305\n",
            "Epoch 103/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4258 - accuracy: 0.5295 - val_loss: 1.4189 - val_accuracy: 0.5265\n",
            "Epoch 104/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.4147 - accuracy: 0.5316 - val_loss: 1.4194 - val_accuracy: 0.5321\n",
            "Epoch 105/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4068 - accuracy: 0.5330 - val_loss: 1.4214 - val_accuracy: 0.5277\n",
            "Epoch 106/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4160 - accuracy: 0.5337 - val_loss: 1.4111 - val_accuracy: 0.5301\n",
            "Epoch 107/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4184 - accuracy: 0.5324 - val_loss: 1.4148 - val_accuracy: 0.5259\n",
            "Epoch 108/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4050 - accuracy: 0.5358 - val_loss: 1.4173 - val_accuracy: 0.5260\n",
            "Epoch 109/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4150 - accuracy: 0.5335 - val_loss: 1.4186 - val_accuracy: 0.5274\n",
            "Epoch 110/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4132 - accuracy: 0.5368 - val_loss: 1.4123 - val_accuracy: 0.5292\n",
            "Epoch 111/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4183 - accuracy: 0.5341 - val_loss: 1.4140 - val_accuracy: 0.5297\n",
            "Epoch 112/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4091 - accuracy: 0.5363 - val_loss: 1.4079 - val_accuracy: 0.5303\n",
            "Epoch 113/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.4071 - accuracy: 0.5367 - val_loss: 1.4162 - val_accuracy: 0.5283\n",
            "Epoch 114/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4026 - accuracy: 0.5396 - val_loss: 1.4213 - val_accuracy: 0.5263\n",
            "Epoch 115/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4084 - accuracy: 0.5350 - val_loss: 1.4194 - val_accuracy: 0.5292\n",
            "Epoch 116/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4171 - accuracy: 0.5375 - val_loss: 1.4129 - val_accuracy: 0.5316\n",
            "Epoch 117/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4114 - accuracy: 0.5373 - val_loss: 1.4156 - val_accuracy: 0.5288\n",
            "Epoch 118/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4098 - accuracy: 0.5356 - val_loss: 1.4160 - val_accuracy: 0.5242\n",
            "Epoch 119/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4085 - accuracy: 0.5370 - val_loss: 1.4190 - val_accuracy: 0.5271\n",
            "Epoch 120/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4307 - accuracy: 0.5289 - val_loss: 1.4282 - val_accuracy: 0.5229\n",
            "Epoch 121/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4219 - accuracy: 0.5333 - val_loss: 1.4152 - val_accuracy: 0.5271\n",
            "Epoch 122/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4143 - accuracy: 0.5361 - val_loss: 1.4227 - val_accuracy: 0.5270\n",
            "Epoch 123/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4164 - accuracy: 0.5343 - val_loss: 1.4205 - val_accuracy: 0.5268\n",
            "Epoch 124/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4229 - accuracy: 0.5337 - val_loss: 1.4114 - val_accuracy: 0.5311\n",
            "Epoch 125/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4042 - accuracy: 0.5394 - val_loss: 1.4099 - val_accuracy: 0.5301\n",
            "Epoch 126/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3993 - accuracy: 0.5419 - val_loss: 1.4116 - val_accuracy: 0.5267\n",
            "Epoch 127/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4094 - accuracy: 0.5401 - val_loss: 1.4165 - val_accuracy: 0.5281\n",
            "Epoch 128/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4081 - accuracy: 0.5419 - val_loss: 1.4228 - val_accuracy: 0.5252\n",
            "Epoch 129/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4022 - accuracy: 0.5422 - val_loss: 1.4063 - val_accuracy: 0.5304\n",
            "Epoch 130/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4140 - accuracy: 0.5380 - val_loss: 1.4202 - val_accuracy: 0.5300\n",
            "Epoch 131/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4011 - accuracy: 0.5390 - val_loss: 1.4142 - val_accuracy: 0.5297\n",
            "Epoch 132/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4048 - accuracy: 0.5416 - val_loss: 1.4129 - val_accuracy: 0.5276\n",
            "Epoch 133/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4032 - accuracy: 0.5408 - val_loss: 1.4255 - val_accuracy: 0.5244\n",
            "Epoch 134/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.4072 - accuracy: 0.5395 - val_loss: 1.4136 - val_accuracy: 0.5290\n",
            "Epoch 135/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4079 - accuracy: 0.5385 - val_loss: 1.4120 - val_accuracy: 0.5316\n",
            "Epoch 136/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4003 - accuracy: 0.5422 - val_loss: 1.4182 - val_accuracy: 0.5303\n",
            "Epoch 137/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3995 - accuracy: 0.5394 - val_loss: 1.4191 - val_accuracy: 0.5286\n",
            "Epoch 138/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4007 - accuracy: 0.5430 - val_loss: 1.4189 - val_accuracy: 0.5301\n",
            "Epoch 139/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4040 - accuracy: 0.5422 - val_loss: 1.4080 - val_accuracy: 0.5321\n",
            "Epoch 140/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3944 - accuracy: 0.5463 - val_loss: 1.4164 - val_accuracy: 0.5303\n",
            "Epoch 141/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3976 - accuracy: 0.5436 - val_loss: 1.4173 - val_accuracy: 0.5301\n",
            "Epoch 142/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4079 - accuracy: 0.5399 - val_loss: 1.4190 - val_accuracy: 0.5242\n",
            "Epoch 143/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3952 - accuracy: 0.5439 - val_loss: 1.4171 - val_accuracy: 0.5297\n",
            "Epoch 144/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4028 - accuracy: 0.5438 - val_loss: 1.4220 - val_accuracy: 0.5256\n",
            "Epoch 145/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3875 - accuracy: 0.5431 - val_loss: 1.4165 - val_accuracy: 0.5377\n",
            "Epoch 146/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3983 - accuracy: 0.5436 - val_loss: 1.4141 - val_accuracy: 0.5348\n",
            "Epoch 147/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4061 - accuracy: 0.5419 - val_loss: 1.4200 - val_accuracy: 0.5291\n",
            "Epoch 148/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3967 - accuracy: 0.5441 - val_loss: 1.4136 - val_accuracy: 0.5296\n",
            "Epoch 149/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4028 - accuracy: 0.5410 - val_loss: 1.4143 - val_accuracy: 0.5279\n",
            "Epoch 150/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4047 - accuracy: 0.5413 - val_loss: 1.4209 - val_accuracy: 0.5278\n",
            "Epoch 151/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3930 - accuracy: 0.5470 - val_loss: 1.4189 - val_accuracy: 0.5269\n",
            "Epoch 152/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4003 - accuracy: 0.5431 - val_loss: 1.4124 - val_accuracy: 0.5296\n",
            "Epoch 153/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3929 - accuracy: 0.5441 - val_loss: 1.4181 - val_accuracy: 0.5277\n",
            "Epoch 154/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3856 - accuracy: 0.5493 - val_loss: 1.4180 - val_accuracy: 0.5303\n",
            "Epoch 155/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3969 - accuracy: 0.5446 - val_loss: 1.4161 - val_accuracy: 0.5276\n",
            "Epoch 156/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3919 - accuracy: 0.5450 - val_loss: 1.4173 - val_accuracy: 0.5287\n",
            "Epoch 157/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3890 - accuracy: 0.5450 - val_loss: 1.4092 - val_accuracy: 0.5324\n",
            "Epoch 158/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3964 - accuracy: 0.5432 - val_loss: 1.4142 - val_accuracy: 0.5286\n",
            "Epoch 159/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4027 - accuracy: 0.5428 - val_loss: 1.4256 - val_accuracy: 0.5270\n",
            "Epoch 160/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3898 - accuracy: 0.5488 - val_loss: 1.4266 - val_accuracy: 0.5253\n",
            "Epoch 161/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3948 - accuracy: 0.5448 - val_loss: 1.4280 - val_accuracy: 0.5226\n",
            "Epoch 162/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4020 - accuracy: 0.5441 - val_loss: 1.4226 - val_accuracy: 0.5293\n",
            "Epoch 163/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4004 - accuracy: 0.5448 - val_loss: 1.4213 - val_accuracy: 0.5284\n",
            "Epoch 164/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3856 - accuracy: 0.5479 - val_loss: 1.4234 - val_accuracy: 0.5332\n",
            "Epoch 165/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3935 - accuracy: 0.5454 - val_loss: 1.4192 - val_accuracy: 0.5290\n",
            "Epoch 166/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3835 - accuracy: 0.5491 - val_loss: 1.4155 - val_accuracy: 0.5300\n",
            "Epoch 167/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3900 - accuracy: 0.5491 - val_loss: 1.4184 - val_accuracy: 0.5311\n",
            "Epoch 168/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3925 - accuracy: 0.5495 - val_loss: 1.4140 - val_accuracy: 0.5331\n",
            "Epoch 169/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3953 - accuracy: 0.5465 - val_loss: 1.4186 - val_accuracy: 0.5287\n",
            "Epoch 170/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3878 - accuracy: 0.5486 - val_loss: 1.4194 - val_accuracy: 0.5330\n",
            "Epoch 171/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3827 - accuracy: 0.5512 - val_loss: 1.4205 - val_accuracy: 0.5320\n",
            "Epoch 172/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3784 - accuracy: 0.5533 - val_loss: 1.4205 - val_accuracy: 0.5306\n",
            "Epoch 173/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3923 - accuracy: 0.5467 - val_loss: 1.4280 - val_accuracy: 0.5249\n",
            "Epoch 174/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3951 - accuracy: 0.5447 - val_loss: 1.4215 - val_accuracy: 0.5291\n",
            "Epoch 175/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3858 - accuracy: 0.5493 - val_loss: 1.4199 - val_accuracy: 0.5316\n",
            "Epoch 176/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3832 - accuracy: 0.5508 - val_loss: 1.4239 - val_accuracy: 0.5286\n",
            "Epoch 177/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3851 - accuracy: 0.5511 - val_loss: 1.4187 - val_accuracy: 0.5306\n",
            "Epoch 178/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3941 - accuracy: 0.5473 - val_loss: 1.4229 - val_accuracy: 0.5325\n",
            "Epoch 179/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3807 - accuracy: 0.5545 - val_loss: 1.4272 - val_accuracy: 0.5270\n",
            "Epoch 180/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4019 - accuracy: 0.5418 - val_loss: 1.4240 - val_accuracy: 0.5268\n",
            "Epoch 181/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3832 - accuracy: 0.5499 - val_loss: 1.4263 - val_accuracy: 0.5287\n",
            "Epoch 182/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3842 - accuracy: 0.5512 - val_loss: 1.4205 - val_accuracy: 0.5299\n",
            "Epoch 183/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3939 - accuracy: 0.5487 - val_loss: 1.4191 - val_accuracy: 0.5331\n",
            "Epoch 184/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.3911 - accuracy: 0.5461 - val_loss: 1.4316 - val_accuracy: 0.5257\n",
            "Epoch 185/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3769 - accuracy: 0.5523 - val_loss: 1.4249 - val_accuracy: 0.5333\n",
            "Epoch 186/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3908 - accuracy: 0.5479 - val_loss: 1.4313 - val_accuracy: 0.5270\n",
            "Epoch 187/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3882 - accuracy: 0.5466 - val_loss: 1.4262 - val_accuracy: 0.5284\n",
            "Epoch 188/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3801 - accuracy: 0.5515 - val_loss: 1.4323 - val_accuracy: 0.5293\n",
            "Epoch 189/200\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.3786 - accuracy: 0.5529 - val_loss: 1.4270 - val_accuracy: 0.5296\n",
            "Epoch 190/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3782 - accuracy: 0.5533 - val_loss: 1.4286 - val_accuracy: 0.5278\n",
            "Epoch 191/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.3793 - accuracy: 0.5537 - val_loss: 1.4287 - val_accuracy: 0.5284\n",
            "Epoch 192/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3813 - accuracy: 0.5503 - val_loss: 1.4254 - val_accuracy: 0.5262\n",
            "Epoch 193/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.3935 - accuracy: 0.5486 - val_loss: 1.4255 - val_accuracy: 0.5292\n",
            "Epoch 194/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3896 - accuracy: 0.5471 - val_loss: 1.4300 - val_accuracy: 0.5273\n",
            "Epoch 195/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3830 - accuracy: 0.5500 - val_loss: 1.4309 - val_accuracy: 0.5303\n",
            "Epoch 196/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.3844 - accuracy: 0.5503 - val_loss: 1.4230 - val_accuracy: 0.5283\n",
            "Epoch 197/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.3698 - accuracy: 0.5547 - val_loss: 1.4283 - val_accuracy: 0.5288\n",
            "Epoch 198/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.3747 - accuracy: 0.5524 - val_loss: 1.4314 - val_accuracy: 0.5242\n",
            "Epoch 199/200\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 1.3823 - accuracy: 0.5521 - val_loss: 1.4262 - val_accuracy: 0.5281\n",
            "Epoch 200/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3938 - accuracy: 0.5492 - val_loss: 1.4246 - val_accuracy: 0.5299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Amk91Chjzqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "e7e07761-58b5-498c-ee6c-9b98a6aabf63"
      },
      "source": [
        "# Plot the training loss and validation loss\n",
        "print(history.history)\n",
        "model.load_weights('/tmp/checkpoint')\n",
        "print(model.evaluate(x_train2,y_train2,verbose=0))\n",
        "print(model.evaluate(x_test2,y_test2,verbose=0))\n",
        "loss_train = np.array(history.history['loss'])\n",
        "loss_test = np.array(history.history['val_loss'])\n",
        "x = np.arange(0,loss_train.shape[0])\n",
        "plt.plot(x,loss_train,label='Training Loss')\n",
        "plt.plot(x,loss_test,label='Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Training Loss','Validation Loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [2.4112794399261475, 2.1312310695648193, 2.031327962875366, 1.9739890098571777, 1.9211370944976807, 1.8796297311782837, 1.8470510244369507, 1.824052333831787, 1.7998156547546387, 1.7811973094940186, 1.7640777826309204, 1.7484078407287598, 1.732393503189087, 1.718002200126648, 1.7015628814697266, 1.6898163557052612, 1.6750880479812622, 1.6798224449157715, 1.6600641012191772, 1.6476765871047974, 1.6362560987472534, 1.6300286054611206, 1.6261723041534424, 1.6167726516723633, 1.5996986627578735, 1.5948455333709717, 1.5994102954864502, 1.5805453062057495, 1.5785157680511475, 1.5695769786834717, 1.5657298564910889, 1.566394567489624, 1.5588154792785645, 1.5516270399093628, 1.5426081418991089, 1.543104648590088, 1.5416035652160645, 1.5360909700393677, 1.5311859846115112, 1.5294404029846191, 1.5243853330612183, 1.5189498662948608, 1.518293023109436, 1.5103123188018799, 1.5103343725204468, 1.5163122415542603, 1.5086883306503296, 1.50252103805542, 1.4921183586120605, 1.49857759475708, 1.493930697441101, 1.489091157913208, 1.4884374141693115, 1.4904792308807373, 1.4879504442214966, 1.4794464111328125, 1.4742733240127563, 1.4844719171524048, 1.471907615661621, 1.4753079414367676, 1.4755680561065674, 1.473421573638916, 1.4735517501831055, 1.4641022682189941, 1.4606388807296753, 1.4550405740737915, 1.4568417072296143, 1.4596641063690186, 1.4618430137634277, 1.4526251554489136, 1.4449784755706787, 1.4462237358093262, 1.4518440961837769, 1.4461034536361694, 1.4354032278060913, 1.4436161518096924, 1.4413200616836548, 1.4424022436141968, 1.4516898393630981, 1.4466357231140137, 1.4339652061462402, 1.4404290914535522, 1.4295693635940552, 1.4381577968597412, 1.4456669092178345, 1.4368563890457153, 1.4334887266159058, 1.42496657371521, 1.4317775964736938, 1.4257917404174805, 1.4343279600143433, 1.4294558763504028, 1.430538296699524, 1.425123691558838, 1.4221001863479614, 1.4126746654510498, 1.4255475997924805, 1.4263837337493896, 1.43260657787323, 1.43012273311615, 1.426150918006897, 1.419487714767456, 1.425795078277588, 1.414687156677246, 1.4068009853363037, 1.4160046577453613, 1.4184303283691406, 1.4050061702728271, 1.415032982826233, 1.4131817817687988, 1.4182822704315186, 1.4091315269470215, 1.4070844650268555, 1.4026472568511963, 1.4084352254867554, 1.4171453714370728, 1.411370038986206, 1.4098304510116577, 1.4085302352905273, 1.4306951761245728, 1.4218876361846924, 1.4143259525299072, 1.416429042816162, 1.4229143857955933, 1.4041551351547241, 1.3993440866470337, 1.4093520641326904, 1.4081230163574219, 1.4021751880645752, 1.4139573574066162, 1.401123046875, 1.4047551155090332, 1.4032150506973267, 1.40719735622406, 1.4078747034072876, 1.4002856016159058, 1.399461269378662, 1.4006993770599365, 1.4040098190307617, 1.3944374322891235, 1.397628664970398, 1.4078547954559326, 1.3951637744903564, 1.4027975797653198, 1.387510895729065, 1.3982577323913574, 1.4060564041137695, 1.3966805934906006, 1.4027611017227173, 1.4046841859817505, 1.3930057287216187, 1.400307059288025, 1.392866611480713, 1.3856269121170044, 1.396867036819458, 1.3919163942337036, 1.389020562171936, 1.39643394947052, 1.4027462005615234, 1.3898283243179321, 1.394798755645752, 1.4020318984985352, 1.4004117250442505, 1.3855901956558228, 1.3935025930404663, 1.3834599256515503, 1.3900117874145508, 1.392538070678711, 1.3953032493591309, 1.3878300189971924, 1.3827146291732788, 1.3783974647521973, 1.3922678232192993, 1.3951259851455688, 1.3857579231262207, 1.3832290172576904, 1.3850562572479248, 1.3941296339035034, 1.3807358741760254, 1.4019232988357544, 1.3831557035446167, 1.3841885328292847, 1.393946886062622, 1.3910770416259766, 1.3768683671951294, 1.3908398151397705, 1.3881652355194092, 1.3800742626190186, 1.3786331415176392, 1.3782292604446411, 1.3793293237686157, 1.381349802017212, 1.393535852432251, 1.3895608186721802, 1.382973074913025, 1.3843598365783691, 1.3697755336761475, 1.3746939897537231, 1.3822715282440186, 1.3937503099441528], 'accuracy': [0.20424999296665192, 0.2642750144004822, 0.289249986410141, 0.30660000443458557, 0.3226749897003174, 0.3375999927520752, 0.34860000014305115, 0.3519749939441681, 0.36489999294281006, 0.37097498774528503, 0.37770000100135803, 0.3847000002861023, 0.391525000333786, 0.39434999227523804, 0.39739999175071716, 0.4080750048160553, 0.4128749966621399, 0.4113999903202057, 0.4225499927997589, 0.4271000027656555, 0.43062499165534973, 0.4321500062942505, 0.4340499937534332, 0.4404500126838684, 0.44394999742507935, 0.4433000087738037, 0.4455749988555908, 0.4491249918937683, 0.4548250138759613, 0.46059998869895935, 0.45627498626708984, 0.46230000257492065, 0.458950012922287, 0.4642750024795532, 0.46822500228881836, 0.47049999237060547, 0.4707750082015991, 0.4762499928474426, 0.4765250086784363, 0.4752750098705292, 0.47540000081062317, 0.4828000068664551, 0.4830999970436096, 0.48315000534057617, 0.48842498660087585, 0.4876750111579895, 0.4873250126838684, 0.48829999566078186, 0.49410000443458557, 0.49285000562667847, 0.49524998664855957, 0.49399998784065247, 0.49707499146461487, 0.4953249990940094, 0.4993250072002411, 0.5038750171661377, 0.501924991607666, 0.5002999901771545, 0.5055000185966492, 0.5015000104904175, 0.5047500133514404, 0.5059999823570251, 0.5055999755859375, 0.5073249936103821, 0.5080999732017517, 0.5118250250816345, 0.5092250108718872, 0.510450005531311, 0.5094749927520752, 0.5149000287055969, 0.5187249779701233, 0.5139750242233276, 0.5135999917984009, 0.5159749984741211, 0.5219749808311462, 0.5179749727249146, 0.5214499831199646, 0.51910001039505, 0.5171499848365784, 0.520799994468689, 0.5220999717712402, 0.5199750065803528, 0.5245000123977661, 0.522724986076355, 0.5180749893188477, 0.5241249799728394, 0.524025022983551, 0.525825023651123, 0.522724986076355, 0.5281749963760376, 0.5234000086784363, 0.5264750123023987, 0.5228250026702881, 0.5290499925613403, 0.5292999744415283, 0.5350499749183655, 0.5274999737739563, 0.5281500220298767, 0.5254999995231628, 0.5249000191688538, 0.5290250182151794, 0.5314249992370605, 0.5295000076293945, 0.5315750241279602, 0.5329999923706055, 0.5336750149726868, 0.5323749780654907, 0.5357999801635742, 0.5335249900817871, 0.5368250012397766, 0.5341249704360962, 0.5362750291824341, 0.5366500020027161, 0.5396249890327454, 0.534974992275238, 0.5374500155448914, 0.5373250246047974, 0.5355749726295471, 0.5370000004768372, 0.5289000272750854, 0.5333250164985657, 0.5360999703407288, 0.5343499779701233, 0.5337499976158142, 0.5393999814987183, 0.5419250130653381, 0.540149986743927, 0.541949987411499, 0.5421500205993652, 0.537975013256073, 0.5390250086784363, 0.541575014591217, 0.5407999753952026, 0.5395249724388123, 0.5385249853134155, 0.5422000288963318, 0.5393999814987183, 0.5430499911308289, 0.5422250032424927, 0.5462999939918518, 0.5436499714851379, 0.5399249792098999, 0.5439249873161316, 0.5437750220298767, 0.5430999994277954, 0.5436499714851379, 0.5418750047683716, 0.5440750122070312, 0.5409749746322632, 0.5412750244140625, 0.546999990940094, 0.5430750250816345, 0.5440750122070312, 0.5492500066757202, 0.5446000099182129, 0.5449749827384949, 0.5450000166893005, 0.5432000160217285, 0.5428249835968018, 0.548799991607666, 0.54482501745224, 0.5441499948501587, 0.5447999835014343, 0.5478500127792358, 0.54544997215271, 0.5491250157356262, 0.5491499900817871, 0.5495250225067139, 0.5464500188827515, 0.5486000180244446, 0.5511749982833862, 0.5532500147819519, 0.5467000007629395, 0.544700026512146, 0.5492500066757202, 0.5507500171661377, 0.5510749816894531, 0.5473499894142151, 0.5545499920845032, 0.541824996471405, 0.5498999953269958, 0.5511500239372253, 0.5486500263214111, 0.5461000204086304, 0.5523250102996826, 0.5479249954223633, 0.5465750098228455, 0.5515000224113464, 0.5529000163078308, 0.5532749891281128, 0.553725004196167, 0.5502750277519226, 0.5485749840736389, 0.547124981880188, 0.5500249862670898, 0.5503000020980835, 0.5546500086784363, 0.5523750185966492, 0.5521249771118164, 0.5491750240325928], 'val_loss': [2.410649061203003, 2.0164568424224854, 1.9280791282653809, 1.848473072052002, 1.8087732791900635, 1.7687926292419434, 1.7339874505996704, 1.704851746559143, 1.671414852142334, 1.6516796350479126, 1.619567632675171, 1.6091407537460327, 1.6003479957580566, 1.5875109434127808, 1.5738403797149658, 1.5592252016067505, 1.5523475408554077, 1.5363820791244507, 1.5359975099563599, 1.5204442739486694, 1.5187358856201172, 1.5126402378082275, 1.5070151090621948, 1.496249794960022, 1.4969079494476318, 1.4958686828613281, 1.4945309162139893, 1.4781816005706787, 1.4795340299606323, 1.4713307619094849, 1.4681199789047241, 1.4675757884979248, 1.4667301177978516, 1.457796573638916, 1.4520633220672607, 1.4594507217407227, 1.4491052627563477, 1.4506845474243164, 1.4491136074066162, 1.4446265697479248, 1.442007303237915, 1.4412816762924194, 1.4425510168075562, 1.447944164276123, 1.4418480396270752, 1.4436211585998535, 1.4373937845230103, 1.4350626468658447, 1.434240698814392, 1.4303683042526245, 1.4327234029769897, 1.425025463104248, 1.4279394149780273, 1.4259976148605347, 1.423862338066101, 1.4275472164154053, 1.4206273555755615, 1.4211196899414062, 1.4265938997268677, 1.4269146919250488, 1.4237862825393677, 1.4283066987991333, 1.425358533859253, 1.42324960231781, 1.4217997789382935, 1.4252701997756958, 1.4173519611358643, 1.4178353548049927, 1.422798752784729, 1.4246189594268799, 1.4205278158187866, 1.4110307693481445, 1.4114841222763062, 1.415601372718811, 1.426355004310608, 1.4208745956420898, 1.4142097234725952, 1.411209225654602, 1.413161277770996, 1.420575737953186, 1.4130723476409912, 1.4201775789260864, 1.4187846183776855, 1.411163568496704, 1.4170243740081787, 1.419795274734497, 1.4144648313522339, 1.4176855087280273, 1.4154818058013916, 1.410284399986267, 1.4182519912719727, 1.414657473564148, 1.418708324432373, 1.4190934896469116, 1.412395715713501, 1.4101436138153076, 1.4164350032806396, 1.41642427444458, 1.4232085943222046, 1.4289990663528442, 1.4149316549301147, 1.4130651950836182, 1.4188607931137085, 1.419389247894287, 1.4213858842849731, 1.411060094833374, 1.4147573709487915, 1.4172776937484741, 1.4185694456100464, 1.412340521812439, 1.414005160331726, 1.4078960418701172, 1.4161858558654785, 1.4213483333587646, 1.4194421768188477, 1.4129048585891724, 1.415553331375122, 1.4160455465316772, 1.4189844131469727, 1.4281623363494873, 1.4151601791381836, 1.4227006435394287, 1.4205398559570312, 1.4113733768463135, 1.409938097000122, 1.411566972732544, 1.416540265083313, 1.422835111618042, 1.4063255786895752, 1.420151948928833, 1.4142459630966187, 1.4129103422164917, 1.425529956817627, 1.413593053817749, 1.4119600057601929, 1.4181699752807617, 1.4191468954086304, 1.4189401865005493, 1.4079937934875488, 1.4164022207260132, 1.4172635078430176, 1.418971061706543, 1.4170551300048828, 1.422033429145813, 1.4164762496948242, 1.4140509366989136, 1.4199832677841187, 1.4136415719985962, 1.4143435955047607, 1.4208900928497314, 1.418940544128418, 1.4124103784561157, 1.4180676937103271, 1.4179884195327759, 1.416081428527832, 1.4173423051834106, 1.4092254638671875, 1.4141812324523926, 1.4256420135498047, 1.4265822172164917, 1.4279749393463135, 1.422606110572815, 1.4212558269500732, 1.423356294631958, 1.4192334413528442, 1.4155054092407227, 1.418418288230896, 1.4140362739562988, 1.4185956716537476, 1.419377088546753, 1.4204939603805542, 1.4205023050308228, 1.4279921054840088, 1.4215320348739624, 1.419850468635559, 1.423870325088501, 1.4186646938323975, 1.4229354858398438, 1.4271689653396606, 1.424026370048523, 1.4262562990188599, 1.4204769134521484, 1.4190548658370972, 1.4316387176513672, 1.4248697757720947, 1.431264042854309, 1.4262022972106934, 1.4323176145553589, 1.426961064338684, 1.4286139011383057, 1.4286609888076782, 1.425399661064148, 1.425527572631836, 1.4300256967544556, 1.4309430122375488, 1.4229565858840942, 1.4283018112182617, 1.4313665628433228, 1.4262467622756958, 1.42460036277771], 'val_accuracy': [0.17829999327659607, 0.2937999963760376, 0.3495999872684479, 0.37770000100135803, 0.383899986743927, 0.38830000162124634, 0.4027000069618225, 0.4115999937057495, 0.421999990940094, 0.4275999963283539, 0.4408999979496002, 0.44440001249313354, 0.4496000111103058, 0.44749999046325684, 0.45399999618530273, 0.46299999952316284, 0.461899995803833, 0.46779999136924744, 0.4690000116825104, 0.47589999437332153, 0.4772999882698059, 0.4724000096321106, 0.4812999963760376, 0.4799000024795532, 0.48539999127388, 0.484499990940094, 0.4853000044822693, 0.492900013923645, 0.4918000102043152, 0.49630001187324524, 0.4970000088214874, 0.4984999895095825, 0.49239999055862427, 0.5013999938964844, 0.5027999877929688, 0.5029000043869019, 0.5052000284194946, 0.5016999840736389, 0.5026000142097473, 0.5090000033378601, 0.5113000273704529, 0.5127999782562256, 0.5101000070571899, 0.5048999786376953, 0.5105000138282776, 0.5098000168800354, 0.5117999911308289, 0.5105999708175659, 0.5148000121116638, 0.5175999999046326, 0.5146999955177307, 0.5224000215530396, 0.5195000171661377, 0.5181999802589417, 0.5188999772071838, 0.5224999785423279, 0.5218999981880188, 0.5231999754905701, 0.519599974155426, 0.5196999907493591, 0.5184000134468079, 0.5141000151634216, 0.5206000208854675, 0.5200999975204468, 0.5223000049591064, 0.5217000246047974, 0.5221999883651733, 0.5199999809265137, 0.522599995136261, 0.5214999914169312, 0.5263000130653381, 0.5299999713897705, 0.5303999781608582, 0.526199996471405, 0.5203999876976013, 0.5230000019073486, 0.5267000198364258, 0.5281000137329102, 0.5253999829292297, 0.5270000100135803, 0.5281000137329102, 0.5224000215530396, 0.5254999995231628, 0.5282999873161316, 0.5232999920845032, 0.5253999829292297, 0.5285000205039978, 0.5314000248908997, 0.5267000198364258, 0.5320000052452087, 0.5267999768257141, 0.5270000100135803, 0.5292999744415283, 0.5289999842643738, 0.5291000008583069, 0.5285999774932861, 0.5295000076293945, 0.5264000296592712, 0.5317999720573425, 0.5267000198364258, 0.5267000198364258, 0.5304999947547913, 0.5264999866485596, 0.5321000218391418, 0.5277000069618225, 0.5300999879837036, 0.5259000062942505, 0.5260000228881836, 0.527400016784668, 0.52920001745224, 0.529699981212616, 0.5303000211715698, 0.5282999873161316, 0.5263000130653381, 0.52920001745224, 0.5315999984741211, 0.5288000106811523, 0.5242000222206116, 0.5271000266075134, 0.5228999853134155, 0.5271000266075134, 0.5270000100135803, 0.5267999768257141, 0.5310999751091003, 0.5300999879837036, 0.5267000198364258, 0.5281000137329102, 0.5252000093460083, 0.5303999781608582, 0.5299999713897705, 0.529699981212616, 0.5275999903678894, 0.524399995803833, 0.5289999842643738, 0.5315999984741211, 0.5303000211715698, 0.5285999774932861, 0.5300999879837036, 0.5321000218391418, 0.5303000211715698, 0.5300999879837036, 0.5242000222206116, 0.529699981212616, 0.525600016117096, 0.5376999974250793, 0.5347999930381775, 0.5291000008583069, 0.5296000242233276, 0.527899980545044, 0.5278000235557556, 0.5268999934196472, 0.5296000242233276, 0.5277000069618225, 0.5303000211715698, 0.5275999903678894, 0.5286999940872192, 0.5324000120162964, 0.5285999774932861, 0.5270000100135803, 0.5253000259399414, 0.522599995136261, 0.5292999744415283, 0.5284000039100647, 0.5332000255584717, 0.5289999842643738, 0.5299999713897705, 0.5310999751091003, 0.5331000089645386, 0.5286999940872192, 0.5329999923706055, 0.5320000052452087, 0.5306000113487244, 0.5249000191688538, 0.5291000008583069, 0.5315999984741211, 0.5285999774932861, 0.5306000113487244, 0.5325000286102295, 0.5270000100135803, 0.5267999768257141, 0.5286999940872192, 0.5299000144004822, 0.5331000089645386, 0.5256999731063843, 0.53329998254776, 0.5270000100135803, 0.5284000039100647, 0.5292999744415283, 0.5296000242233276, 0.5278000235557556, 0.5284000039100647, 0.526199996471405, 0.52920001745224, 0.5273000001907349, 0.5303000211715698, 0.5282999873161316, 0.5288000106811523, 0.5242000222206116, 0.5281000137329102, 0.5299000144004822]}\n",
            "[1.1347017288208008, 0.6424800157546997]\n",
            "[1.388664722442627, 0.5376999974250793]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fEbEqLfADJu"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Construct a convolutional neural network using your own structure. Try to maximize the prediction accuracy of your model.\n",
        "\n",
        "After the training process, print the training, validation, and test accuracies, as well as plot the training loss and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTrbgdtZknEM",
        "outputId": "d43d8d66-8573-4250-8396-2560c4598542"
      },
      "source": [
        "# Your implementation for Question 2\n",
        "x_train3 = x_train/255.0\n",
        "x_test3 = x_test/255.0\n",
        "y_train3 = tf.keras.utils.to_categorical(y_train)\n",
        "y_test3 = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "x_train3 = x_train3.reshape((-1,32,32,3))\n",
        "x_test3 = x_test3.reshape((-1,32,32,3))\n",
        "tf.random.set_seed(11)\n",
        "\n",
        "#Define a model\n",
        "model2 = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=(32,32,3)),\n",
        "        tf.keras.layers.Conv2D(128,(3,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((3,3),strides=(2,2)),\n",
        "        tf.keras.layers.Conv2D(256,(3,3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((3,3),strides=(2,2)),\n",
        "        tf.keras.layers.SpatialDropout2D(0.5),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "# complie model\n",
        "model2.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "model2.summary()\n",
        "\n",
        "# set a checkpoint callback & train model\n",
        "model2_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = '/tmp/checkpoint',save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)\n",
        "history2 = model2.fit(x_train3,y_train3,epochs=200,batch_size=1024,shuffle=True,validation_split=0.2,callbacks=[model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 30, 30, 128)       3584      \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 30, 30, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 30, 30, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 12, 12, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 12, 12, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " spatial_dropout2d_1 (Spatia  (None, 5, 5, 256)        0         \n",
            " lDropout2D)                                                     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               3277312   \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,584,778\n",
            "Trainable params: 3,582,986\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "40/40 [==============================] - 13s 271ms/step - loss: 2.0190 - accuracy: 0.3933 - val_loss: 2.7223 - val_accuracy: 0.0952\n",
            "Epoch 2/200\n",
            "40/40 [==============================] - 11s 266ms/step - loss: 1.4958 - accuracy: 0.5169 - val_loss: 3.3317 - val_accuracy: 0.0952\n",
            "Epoch 3/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 1.3423 - accuracy: 0.5691 - val_loss: 3.7759 - val_accuracy: 0.1272\n",
            "Epoch 4/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 1.2622 - accuracy: 0.5981 - val_loss: 4.1016 - val_accuracy: 0.1425\n",
            "Epoch 5/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 1.1994 - accuracy: 0.6186 - val_loss: 4.1183 - val_accuracy: 0.1314\n",
            "Epoch 6/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 1.1354 - accuracy: 0.6446 - val_loss: 3.4276 - val_accuracy: 0.1186\n",
            "Epoch 7/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 1.1041 - accuracy: 0.6542 - val_loss: 3.2011 - val_accuracy: 0.1502\n",
            "Epoch 8/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 1.0625 - accuracy: 0.6703 - val_loss: 3.3390 - val_accuracy: 0.1506\n",
            "Epoch 9/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 1.0222 - accuracy: 0.6835 - val_loss: 2.7259 - val_accuracy: 0.2366\n",
            "Epoch 10/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 1.0117 - accuracy: 0.6866 - val_loss: 2.5052 - val_accuracy: 0.2530\n",
            "Epoch 11/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.9750 - accuracy: 0.6997 - val_loss: 2.0440 - val_accuracy: 0.3035\n",
            "Epoch 12/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.9557 - accuracy: 0.7064 - val_loss: 1.9752 - val_accuracy: 0.4066\n",
            "Epoch 13/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.9532 - accuracy: 0.7061 - val_loss: 1.8577 - val_accuracy: 0.4602\n",
            "Epoch 14/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.9167 - accuracy: 0.7214 - val_loss: 1.5974 - val_accuracy: 0.4991\n",
            "Epoch 15/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.8971 - accuracy: 0.7262 - val_loss: 1.2792 - val_accuracy: 0.5889\n",
            "Epoch 16/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.8821 - accuracy: 0.7307 - val_loss: 1.1231 - val_accuracy: 0.6379\n",
            "Epoch 17/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.8669 - accuracy: 0.7382 - val_loss: 0.9588 - val_accuracy: 0.7020\n",
            "Epoch 18/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.8557 - accuracy: 0.7431 - val_loss: 1.0324 - val_accuracy: 0.6803\n",
            "Epoch 19/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.8324 - accuracy: 0.7493 - val_loss: 0.9527 - val_accuracy: 0.7065\n",
            "Epoch 20/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.8039 - accuracy: 0.7592 - val_loss: 1.0020 - val_accuracy: 0.6899\n",
            "Epoch 21/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.8158 - accuracy: 0.7548 - val_loss: 0.9452 - val_accuracy: 0.7149\n",
            "Epoch 22/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.8198 - accuracy: 0.7554 - val_loss: 0.9540 - val_accuracy: 0.7182\n",
            "Epoch 23/200\n",
            "40/40 [==============================] - 11s 264ms/step - loss: 0.7972 - accuracy: 0.7621 - val_loss: 0.8663 - val_accuracy: 0.7442\n",
            "Epoch 24/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.7718 - accuracy: 0.7716 - val_loss: 0.8590 - val_accuracy: 0.7493\n",
            "Epoch 25/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.7427 - accuracy: 0.7836 - val_loss: 0.9131 - val_accuracy: 0.7275\n",
            "Epoch 26/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.7440 - accuracy: 0.7832 - val_loss: 0.9649 - val_accuracy: 0.7090\n",
            "Epoch 27/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.7268 - accuracy: 0.7885 - val_loss: 0.9426 - val_accuracy: 0.7262\n",
            "Epoch 28/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.7294 - accuracy: 0.7854 - val_loss: 1.1327 - val_accuracy: 0.6608\n",
            "Epoch 29/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.7164 - accuracy: 0.7927 - val_loss: 0.9324 - val_accuracy: 0.7251\n",
            "Epoch 30/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.7145 - accuracy: 0.7947 - val_loss: 0.9256 - val_accuracy: 0.7255\n",
            "Epoch 31/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6960 - accuracy: 0.8002 - val_loss: 0.9529 - val_accuracy: 0.7236\n",
            "Epoch 32/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6836 - accuracy: 0.8061 - val_loss: 0.8042 - val_accuracy: 0.7751\n",
            "Epoch 33/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.6736 - accuracy: 0.8114 - val_loss: 0.8165 - val_accuracy: 0.7630\n",
            "Epoch 34/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6676 - accuracy: 0.8114 - val_loss: 1.1416 - val_accuracy: 0.6599\n",
            "Epoch 35/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6703 - accuracy: 0.8135 - val_loss: 1.0062 - val_accuracy: 0.7127\n",
            "Epoch 36/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.6507 - accuracy: 0.8185 - val_loss: 0.8531 - val_accuracy: 0.7634\n",
            "Epoch 37/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6524 - accuracy: 0.8199 - val_loss: 1.4423 - val_accuracy: 0.6118\n",
            "Epoch 38/200\n",
            "40/40 [==============================] - 11s 266ms/step - loss: 0.6353 - accuracy: 0.8268 - val_loss: 0.8207 - val_accuracy: 0.7691\n",
            "Epoch 39/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6304 - accuracy: 0.8272 - val_loss: 0.8195 - val_accuracy: 0.7674\n",
            "Epoch 40/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6213 - accuracy: 0.8332 - val_loss: 1.0031 - val_accuracy: 0.7169\n",
            "Epoch 41/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.6338 - accuracy: 0.8321 - val_loss: 0.8809 - val_accuracy: 0.7492\n",
            "Epoch 42/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.5943 - accuracy: 0.8457 - val_loss: 0.8574 - val_accuracy: 0.7600\n",
            "Epoch 43/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5786 - accuracy: 0.8482 - val_loss: 0.9126 - val_accuracy: 0.7431\n",
            "Epoch 44/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5937 - accuracy: 0.8436 - val_loss: 0.8174 - val_accuracy: 0.7722\n",
            "Epoch 45/200\n",
            "40/40 [==============================] - 10s 249ms/step - loss: 0.5751 - accuracy: 0.8516 - val_loss: 0.7896 - val_accuracy: 0.7845\n",
            "Epoch 46/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.5657 - accuracy: 0.8558 - val_loss: 1.3281 - val_accuracy: 0.6387\n",
            "Epoch 47/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5568 - accuracy: 0.8601 - val_loss: 0.8987 - val_accuracy: 0.7457\n",
            "Epoch 48/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.5672 - accuracy: 0.8559 - val_loss: 0.9756 - val_accuracy: 0.7313\n",
            "Epoch 49/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5503 - accuracy: 0.8630 - val_loss: 0.9167 - val_accuracy: 0.7473\n",
            "Epoch 50/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5526 - accuracy: 0.8624 - val_loss: 0.8055 - val_accuracy: 0.7839\n",
            "Epoch 51/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5334 - accuracy: 0.8715 - val_loss: 0.8250 - val_accuracy: 0.7775\n",
            "Epoch 52/200\n",
            "40/40 [==============================] - 10s 249ms/step - loss: 0.5392 - accuracy: 0.8697 - val_loss: 0.8913 - val_accuracy: 0.7619\n",
            "Epoch 53/200\n",
            "40/40 [==============================] - 10s 249ms/step - loss: 0.5510 - accuracy: 0.8662 - val_loss: 0.8360 - val_accuracy: 0.7786\n",
            "Epoch 54/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.5229 - accuracy: 0.8761 - val_loss: 0.9389 - val_accuracy: 0.7452\n",
            "Epoch 55/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5449 - accuracy: 0.8714 - val_loss: 0.8752 - val_accuracy: 0.7642\n",
            "Epoch 56/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.5116 - accuracy: 0.8831 - val_loss: 0.9053 - val_accuracy: 0.7535\n",
            "Epoch 57/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.5086 - accuracy: 0.8832 - val_loss: 0.9015 - val_accuracy: 0.7577\n",
            "Epoch 58/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4998 - accuracy: 0.8885 - val_loss: 0.9235 - val_accuracy: 0.7477\n",
            "Epoch 59/200\n",
            "40/40 [==============================] - 10s 249ms/step - loss: 0.4943 - accuracy: 0.8892 - val_loss: 0.9240 - val_accuracy: 0.7497\n",
            "Epoch 60/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.5115 - accuracy: 0.8852 - val_loss: 0.8909 - val_accuracy: 0.7632\n",
            "Epoch 61/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.5125 - accuracy: 0.8849 - val_loss: 0.8815 - val_accuracy: 0.7704\n",
            "Epoch 62/200\n",
            "40/40 [==============================] - 10s 249ms/step - loss: 0.4976 - accuracy: 0.8925 - val_loss: 0.8430 - val_accuracy: 0.7799\n",
            "Epoch 63/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4909 - accuracy: 0.8938 - val_loss: 0.8220 - val_accuracy: 0.7881\n",
            "Epoch 64/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4755 - accuracy: 0.8999 - val_loss: 0.8262 - val_accuracy: 0.7878\n",
            "Epoch 65/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4885 - accuracy: 0.8949 - val_loss: 0.8523 - val_accuracy: 0.7826\n",
            "Epoch 66/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4656 - accuracy: 0.9037 - val_loss: 0.9536 - val_accuracy: 0.7468\n",
            "Epoch 67/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4756 - accuracy: 0.9019 - val_loss: 0.9019 - val_accuracy: 0.7745\n",
            "Epoch 68/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4633 - accuracy: 0.9046 - val_loss: 0.9043 - val_accuracy: 0.7703\n",
            "Epoch 69/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4647 - accuracy: 0.9072 - val_loss: 0.9910 - val_accuracy: 0.7466\n",
            "Epoch 70/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4643 - accuracy: 0.9049 - val_loss: 0.9706 - val_accuracy: 0.7459\n",
            "Epoch 71/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4672 - accuracy: 0.9063 - val_loss: 0.9089 - val_accuracy: 0.7717\n",
            "Epoch 72/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4882 - accuracy: 0.9004 - val_loss: 0.9417 - val_accuracy: 0.7620\n",
            "Epoch 73/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4887 - accuracy: 0.9006 - val_loss: 1.0742 - val_accuracy: 0.7383\n",
            "Epoch 74/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4724 - accuracy: 0.9084 - val_loss: 1.0382 - val_accuracy: 0.7389\n",
            "Epoch 75/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4697 - accuracy: 0.9074 - val_loss: 1.0546 - val_accuracy: 0.7397\n",
            "Epoch 76/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4748 - accuracy: 0.9072 - val_loss: 0.9497 - val_accuracy: 0.7639\n",
            "Epoch 77/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4794 - accuracy: 0.9061 - val_loss: 0.9527 - val_accuracy: 0.7678\n",
            "Epoch 78/200\n",
            "40/40 [==============================] - 11s 268ms/step - loss: 0.4743 - accuracy: 0.9089 - val_loss: 0.8581 - val_accuracy: 0.7947\n",
            "Epoch 79/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4565 - accuracy: 0.9152 - val_loss: 0.9111 - val_accuracy: 0.7807\n",
            "Epoch 80/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4406 - accuracy: 0.9208 - val_loss: 1.0063 - val_accuracy: 0.7506\n",
            "Epoch 81/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4341 - accuracy: 0.9211 - val_loss: 0.8790 - val_accuracy: 0.7909\n",
            "Epoch 82/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4223 - accuracy: 0.9283 - val_loss: 0.9093 - val_accuracy: 0.7768\n",
            "Epoch 83/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4338 - accuracy: 0.9202 - val_loss: 0.8806 - val_accuracy: 0.7827\n",
            "Epoch 84/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4539 - accuracy: 0.9168 - val_loss: 0.9225 - val_accuracy: 0.7743\n",
            "Epoch 85/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4294 - accuracy: 0.9254 - val_loss: 0.8956 - val_accuracy: 0.7857\n",
            "Epoch 86/200\n",
            "40/40 [==============================] - 11s 265ms/step - loss: 0.4287 - accuracy: 0.9270 - val_loss: 0.9067 - val_accuracy: 0.7854\n",
            "Epoch 87/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4379 - accuracy: 0.9223 - val_loss: 0.9902 - val_accuracy: 0.7647\n",
            "Epoch 88/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4712 - accuracy: 0.9110 - val_loss: 0.9955 - val_accuracy: 0.7641\n",
            "Epoch 89/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4428 - accuracy: 0.9221 - val_loss: 0.9147 - val_accuracy: 0.7766\n",
            "Epoch 90/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4393 - accuracy: 0.9234 - val_loss: 1.1071 - val_accuracy: 0.7371\n",
            "Epoch 91/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4373 - accuracy: 0.9235 - val_loss: 0.9114 - val_accuracy: 0.7780\n",
            "Epoch 92/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4467 - accuracy: 0.9201 - val_loss: 0.9023 - val_accuracy: 0.7885\n",
            "Epoch 93/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4521 - accuracy: 0.9204 - val_loss: 0.9028 - val_accuracy: 0.7898\n",
            "Epoch 94/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4138 - accuracy: 0.9333 - val_loss: 0.9303 - val_accuracy: 0.7722\n",
            "Epoch 95/200\n",
            "40/40 [==============================] - 10s 253ms/step - loss: 0.4184 - accuracy: 0.9315 - val_loss: 0.8720 - val_accuracy: 0.7957\n",
            "Epoch 96/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4174 - accuracy: 0.9305 - val_loss: 1.0212 - val_accuracy: 0.7598\n",
            "Epoch 97/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4177 - accuracy: 0.9323 - val_loss: 0.8961 - val_accuracy: 0.7931\n",
            "Epoch 98/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.4350 - accuracy: 0.9260 - val_loss: 0.9719 - val_accuracy: 0.7622\n",
            "Epoch 99/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4358 - accuracy: 0.9275 - val_loss: 1.0718 - val_accuracy: 0.7438\n",
            "Epoch 100/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4787 - accuracy: 0.9127 - val_loss: 0.8863 - val_accuracy: 0.7945\n",
            "Epoch 101/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4435 - accuracy: 0.9268 - val_loss: 0.9478 - val_accuracy: 0.7848\n",
            "Epoch 102/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4250 - accuracy: 0.9324 - val_loss: 0.9933 - val_accuracy: 0.7678\n",
            "Epoch 103/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4325 - accuracy: 0.9305 - val_loss: 0.8989 - val_accuracy: 0.7947\n",
            "Epoch 104/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4305 - accuracy: 0.9309 - val_loss: 0.9744 - val_accuracy: 0.7732\n",
            "Epoch 105/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4369 - accuracy: 0.9287 - val_loss: 0.9600 - val_accuracy: 0.7799\n",
            "Epoch 106/200\n",
            "40/40 [==============================] - 11s 266ms/step - loss: 0.4173 - accuracy: 0.9344 - val_loss: 0.9435 - val_accuracy: 0.7873\n",
            "Epoch 107/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4187 - accuracy: 0.9329 - val_loss: 0.9367 - val_accuracy: 0.7882\n",
            "Epoch 108/200\n",
            "40/40 [==============================] - 10s 253ms/step - loss: 0.4125 - accuracy: 0.9373 - val_loss: 0.8574 - val_accuracy: 0.8057\n",
            "Epoch 109/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4047 - accuracy: 0.9396 - val_loss: 0.8652 - val_accuracy: 0.8013\n",
            "Epoch 110/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3943 - accuracy: 0.9418 - val_loss: 0.9033 - val_accuracy: 0.7991\n",
            "Epoch 111/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4040 - accuracy: 0.9387 - val_loss: 0.9201 - val_accuracy: 0.7916\n",
            "Epoch 112/200\n",
            "40/40 [==============================] - 11s 265ms/step - loss: 0.4400 - accuracy: 0.9269 - val_loss: 0.9325 - val_accuracy: 0.7807\n",
            "Epoch 113/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4280 - accuracy: 0.9314 - val_loss: 0.9280 - val_accuracy: 0.7888\n",
            "Epoch 114/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4160 - accuracy: 0.9362 - val_loss: 0.9350 - val_accuracy: 0.7905\n",
            "Epoch 115/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4295 - accuracy: 0.9321 - val_loss: 0.9352 - val_accuracy: 0.7908\n",
            "Epoch 116/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4056 - accuracy: 0.9403 - val_loss: 0.9503 - val_accuracy: 0.7847\n",
            "Epoch 117/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4148 - accuracy: 0.9373 - val_loss: 0.9757 - val_accuracy: 0.7910\n",
            "Epoch 118/200\n",
            "40/40 [==============================] - 11s 266ms/step - loss: 0.4217 - accuracy: 0.9349 - val_loss: 1.1970 - val_accuracy: 0.7286\n",
            "Epoch 119/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4203 - accuracy: 0.9359 - val_loss: 0.9555 - val_accuracy: 0.7827\n",
            "Epoch 120/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4095 - accuracy: 0.9402 - val_loss: 0.8974 - val_accuracy: 0.7945\n",
            "Epoch 121/200\n",
            "40/40 [==============================] - 10s 249ms/step - loss: 0.3940 - accuracy: 0.9441 - val_loss: 0.8906 - val_accuracy: 0.7995\n",
            "Epoch 122/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3971 - accuracy: 0.9437 - val_loss: 0.9448 - val_accuracy: 0.7839\n",
            "Epoch 123/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.3936 - accuracy: 0.9435 - val_loss: 0.8908 - val_accuracy: 0.7988\n",
            "Epoch 124/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.3936 - accuracy: 0.9460 - val_loss: 0.9652 - val_accuracy: 0.7770\n",
            "Epoch 125/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.3951 - accuracy: 0.9429 - val_loss: 0.9985 - val_accuracy: 0.7712\n",
            "Epoch 126/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.3894 - accuracy: 0.9459 - val_loss: 0.9098 - val_accuracy: 0.7953\n",
            "Epoch 127/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.3764 - accuracy: 0.9495 - val_loss: 1.1074 - val_accuracy: 0.7456\n",
            "Epoch 128/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4197 - accuracy: 0.9362 - val_loss: 1.0379 - val_accuracy: 0.7587\n",
            "Epoch 129/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4146 - accuracy: 0.9374 - val_loss: 0.9590 - val_accuracy: 0.7851\n",
            "Epoch 130/200\n",
            "40/40 [==============================] - 11s 265ms/step - loss: 0.3975 - accuracy: 0.9432 - val_loss: 1.0486 - val_accuracy: 0.7625\n",
            "Epoch 131/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4277 - accuracy: 0.9356 - val_loss: 0.8919 - val_accuracy: 0.7944\n",
            "Epoch 132/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4023 - accuracy: 0.9420 - val_loss: 0.9302 - val_accuracy: 0.7866\n",
            "Epoch 133/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4049 - accuracy: 0.9421 - val_loss: 0.9132 - val_accuracy: 0.7863\n",
            "Epoch 134/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4091 - accuracy: 0.9407 - val_loss: 1.0175 - val_accuracy: 0.7720\n",
            "Epoch 135/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4089 - accuracy: 0.9404 - val_loss: 0.9657 - val_accuracy: 0.7883\n",
            "Epoch 136/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4086 - accuracy: 0.9419 - val_loss: 0.9752 - val_accuracy: 0.7795\n",
            "Epoch 137/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4085 - accuracy: 0.9420 - val_loss: 0.9421 - val_accuracy: 0.7848\n",
            "Epoch 138/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4105 - accuracy: 0.9415 - val_loss: 0.9915 - val_accuracy: 0.7752\n",
            "Epoch 139/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4269 - accuracy: 0.9348 - val_loss: 0.9059 - val_accuracy: 0.7956\n",
            "Epoch 140/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.3954 - accuracy: 0.9455 - val_loss: 0.9262 - val_accuracy: 0.7861\n",
            "Epoch 141/200\n",
            "40/40 [==============================] - 10s 249ms/step - loss: 0.4215 - accuracy: 0.9375 - val_loss: 0.9212 - val_accuracy: 0.7950\n",
            "Epoch 142/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3848 - accuracy: 0.9509 - val_loss: 0.9495 - val_accuracy: 0.7908\n",
            "Epoch 143/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3930 - accuracy: 0.9460 - val_loss: 0.9829 - val_accuracy: 0.7852\n",
            "Epoch 144/200\n",
            "40/40 [==============================] - 11s 264ms/step - loss: 0.3746 - accuracy: 0.9524 - val_loss: 0.9294 - val_accuracy: 0.7927\n",
            "Epoch 145/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3745 - accuracy: 0.9522 - val_loss: 0.8904 - val_accuracy: 0.7971\n",
            "Epoch 146/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3986 - accuracy: 0.9448 - val_loss: 0.9053 - val_accuracy: 0.7939\n",
            "Epoch 147/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4308 - accuracy: 0.9336 - val_loss: 1.0170 - val_accuracy: 0.7761\n",
            "Epoch 148/200\n",
            "40/40 [==============================] - 11s 265ms/step - loss: 0.4256 - accuracy: 0.9368 - val_loss: 0.9168 - val_accuracy: 0.7973\n",
            "Epoch 149/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4080 - accuracy: 0.9434 - val_loss: 0.9434 - val_accuracy: 0.7901\n",
            "Epoch 150/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4099 - accuracy: 0.9425 - val_loss: 0.9638 - val_accuracy: 0.7885\n",
            "Epoch 151/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4179 - accuracy: 0.9405 - val_loss: 1.0218 - val_accuracy: 0.7739\n",
            "Epoch 152/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.4424 - accuracy: 0.9329 - val_loss: 0.9485 - val_accuracy: 0.7846\n",
            "Epoch 153/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4156 - accuracy: 0.9418 - val_loss: 1.1103 - val_accuracy: 0.7551\n",
            "Epoch 154/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4220 - accuracy: 0.9399 - val_loss: 0.9615 - val_accuracy: 0.7850\n",
            "Epoch 155/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4044 - accuracy: 0.9455 - val_loss: 0.8913 - val_accuracy: 0.8011\n",
            "Epoch 156/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4044 - accuracy: 0.9466 - val_loss: 0.9677 - val_accuracy: 0.7888\n",
            "Epoch 157/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4006 - accuracy: 0.9475 - val_loss: 0.9167 - val_accuracy: 0.7866\n",
            "Epoch 158/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3936 - accuracy: 0.9489 - val_loss: 1.1227 - val_accuracy: 0.7548\n",
            "Epoch 159/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4030 - accuracy: 0.9445 - val_loss: 0.9152 - val_accuracy: 0.7994\n",
            "Epoch 160/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3885 - accuracy: 0.9493 - val_loss: 0.9102 - val_accuracy: 0.7939\n",
            "Epoch 161/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3924 - accuracy: 0.9490 - val_loss: 0.9095 - val_accuracy: 0.7930\n",
            "Epoch 162/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3790 - accuracy: 0.9529 - val_loss: 0.9899 - val_accuracy: 0.7780\n",
            "Epoch 163/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3771 - accuracy: 0.9536 - val_loss: 0.9700 - val_accuracy: 0.7770\n",
            "Epoch 164/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3851 - accuracy: 0.9493 - val_loss: 0.9278 - val_accuracy: 0.7952\n",
            "Epoch 165/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3741 - accuracy: 0.9535 - val_loss: 0.9334 - val_accuracy: 0.7869\n",
            "Epoch 166/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3689 - accuracy: 0.9534 - val_loss: 0.9151 - val_accuracy: 0.7998\n",
            "Epoch 167/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3821 - accuracy: 0.9504 - val_loss: 0.9187 - val_accuracy: 0.8006\n",
            "Epoch 168/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3745 - accuracy: 0.9519 - val_loss: 0.9022 - val_accuracy: 0.8006\n",
            "Epoch 169/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3689 - accuracy: 0.9545 - val_loss: 0.9448 - val_accuracy: 0.7955\n",
            "Epoch 170/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3766 - accuracy: 0.9522 - val_loss: 0.9239 - val_accuracy: 0.7986\n",
            "Epoch 171/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3666 - accuracy: 0.9549 - val_loss: 0.9334 - val_accuracy: 0.8034\n",
            "Epoch 172/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3709 - accuracy: 0.9535 - val_loss: 0.9595 - val_accuracy: 0.7942\n",
            "Epoch 173/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3777 - accuracy: 0.9506 - val_loss: 0.9107 - val_accuracy: 0.7985\n",
            "Epoch 174/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3883 - accuracy: 0.9481 - val_loss: 0.9151 - val_accuracy: 0.7978\n",
            "Epoch 175/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3750 - accuracy: 0.9520 - val_loss: 0.9294 - val_accuracy: 0.7902\n",
            "Epoch 176/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3849 - accuracy: 0.9486 - val_loss: 0.9270 - val_accuracy: 0.7952\n",
            "Epoch 177/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3856 - accuracy: 0.9495 - val_loss: 0.9937 - val_accuracy: 0.7826\n",
            "Epoch 178/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3759 - accuracy: 0.9530 - val_loss: 0.9374 - val_accuracy: 0.7961\n",
            "Epoch 179/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3966 - accuracy: 0.9447 - val_loss: 1.0997 - val_accuracy: 0.7574\n",
            "Epoch 180/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3985 - accuracy: 0.9453 - val_loss: 0.8983 - val_accuracy: 0.7957\n",
            "Epoch 181/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4060 - accuracy: 0.9433 - val_loss: 0.9427 - val_accuracy: 0.7924\n",
            "Epoch 182/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.4093 - accuracy: 0.9422 - val_loss: 1.0254 - val_accuracy: 0.7791\n",
            "Epoch 183/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.4015 - accuracy: 0.9452 - val_loss: 0.9215 - val_accuracy: 0.8015\n",
            "Epoch 184/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3864 - accuracy: 0.9505 - val_loss: 0.9466 - val_accuracy: 0.7937\n",
            "Epoch 185/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3833 - accuracy: 0.9516 - val_loss: 1.0061 - val_accuracy: 0.7808\n",
            "Epoch 186/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3641 - accuracy: 0.9569 - val_loss: 0.8929 - val_accuracy: 0.8057\n",
            "Epoch 187/200\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 0.3848 - accuracy: 0.9497 - val_loss: 1.0104 - val_accuracy: 0.7819\n",
            "Epoch 188/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3926 - accuracy: 0.9479 - val_loss: 1.1737 - val_accuracy: 0.7467\n",
            "Epoch 189/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3830 - accuracy: 0.9514 - val_loss: 1.0055 - val_accuracy: 0.7826\n",
            "Epoch 190/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3790 - accuracy: 0.9523 - val_loss: 1.0133 - val_accuracy: 0.7798\n",
            "Epoch 191/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3791 - accuracy: 0.9532 - val_loss: 0.9134 - val_accuracy: 0.7946\n",
            "Epoch 192/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3805 - accuracy: 0.9520 - val_loss: 0.9609 - val_accuracy: 0.7896\n",
            "Epoch 193/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3689 - accuracy: 0.9558 - val_loss: 0.9094 - val_accuracy: 0.8024\n",
            "Epoch 194/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3673 - accuracy: 0.9552 - val_loss: 0.9968 - val_accuracy: 0.7867\n",
            "Epoch 195/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3753 - accuracy: 0.9536 - val_loss: 1.0014 - val_accuracy: 0.7785\n",
            "Epoch 196/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3796 - accuracy: 0.9531 - val_loss: 1.0169 - val_accuracy: 0.7817\n",
            "Epoch 197/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3832 - accuracy: 0.9503 - val_loss: 0.9286 - val_accuracy: 0.7956\n",
            "Epoch 198/200\n",
            "40/40 [==============================] - 10s 251ms/step - loss: 0.3695 - accuracy: 0.9542 - val_loss: 0.9404 - val_accuracy: 0.7918\n",
            "Epoch 199/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3767 - accuracy: 0.9527 - val_loss: 0.9253 - val_accuracy: 0.7949\n",
            "Epoch 200/200\n",
            "40/40 [==============================] - 10s 252ms/step - loss: 0.3890 - accuracy: 0.9488 - val_loss: 0.9954 - val_accuracy: 0.7798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "nN2z77iAkpft",
        "outputId": "84744394-114d-40b2-bc6a-033f32520851"
      },
      "source": [
        "# Plot the training loss and validation loss\n",
        "print(history2.history)\n",
        "model2.load_weights('/tmp/checkpoint')\n",
        "print(model2.evaluate(x_train3,y_train3,verbose=0))\n",
        "print(model2.evaluate(x_test3,y_test3,verbose=0))\n",
        "loss_train2 = np.array(history2.history['loss'])\n",
        "loss_test2 = np.array(history2.history['val_loss'])\n",
        "x2 = np.arange(0,loss_train2.shape[0])\n",
        "plt.plot(x2,loss_train2,label='Training Loss')\n",
        "plt.plot(x2,loss_test2,label='Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Training Loss','Validation Loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [2.0189852714538574, 1.4957658052444458, 1.3423107862472534, 1.2622488737106323, 1.1993902921676636, 1.1354143619537354, 1.1041409969329834, 1.062524676322937, 1.0222015380859375, 1.0116719007492065, 0.9750111103057861, 0.9557264447212219, 0.9531852602958679, 0.9167073965072632, 0.8970794677734375, 0.882149338722229, 0.866867184638977, 0.8557206988334656, 0.832396388053894, 0.8039259910583496, 0.8158173561096191, 0.8197792768478394, 0.7972015142440796, 0.7718296051025391, 0.7427088618278503, 0.7440065741539001, 0.7267738580703735, 0.729416012763977, 0.7163625955581665, 0.7144774794578552, 0.6960183382034302, 0.6836366653442383, 0.6736224293708801, 0.667607843875885, 0.6702982187271118, 0.6507239937782288, 0.6523803472518921, 0.6353419423103333, 0.6304196715354919, 0.6213194131851196, 0.6338307857513428, 0.5943188667297363, 0.578580379486084, 0.5936596393585205, 0.5751136541366577, 0.5656831860542297, 0.5567961931228638, 0.5671820044517517, 0.5503221154212952, 0.5525999665260315, 0.5334289073944092, 0.5392286777496338, 0.5510002374649048, 0.522896409034729, 0.5449185371398926, 0.511608898639679, 0.5086076855659485, 0.49976035952568054, 0.4943179786205292, 0.5115340352058411, 0.5125433802604675, 0.49760931730270386, 0.49085304141044617, 0.47548770904541016, 0.4884895980358124, 0.46558812260627747, 0.47555941343307495, 0.4633173942565918, 0.46465757489204407, 0.4642695188522339, 0.46716415882110596, 0.4881722033023834, 0.48872655630111694, 0.4724104404449463, 0.46971333026885986, 0.47477084398269653, 0.4794248640537262, 0.4742550849914551, 0.4565250873565674, 0.440550297498703, 0.43413135409355164, 0.42231807112693787, 0.43375518918037415, 0.4538918435573578, 0.4293641746044159, 0.4286561608314514, 0.4378597140312195, 0.471217542886734, 0.442783921957016, 0.43933287262916565, 0.4373070299625397, 0.4466916620731354, 0.45214757323265076, 0.4137515127658844, 0.4183892607688904, 0.4174397885799408, 0.417663037776947, 0.4350408613681793, 0.43575742840766907, 0.47872450947761536, 0.4435308575630188, 0.42498070001602173, 0.43245020508766174, 0.4304570257663727, 0.4368989169597626, 0.4172983765602112, 0.41874730587005615, 0.412476122379303, 0.40471190214157104, 0.3943176865577698, 0.404041051864624, 0.4400395154953003, 0.42798900604248047, 0.41595762968063354, 0.4295341670513153, 0.40555357933044434, 0.4147602915763855, 0.42169228196144104, 0.42030152678489685, 0.4094884991645813, 0.39399397373199463, 0.39712056517601013, 0.3936444818973541, 0.3936416804790497, 0.3951360285282135, 0.389369934797287, 0.376404732465744, 0.4197019636631012, 0.4145548939704895, 0.39748814702033997, 0.42771854996681213, 0.40225717425346375, 0.4048989713191986, 0.40908804535865784, 0.4088667333126068, 0.4086097776889801, 0.4084801971912384, 0.4105055034160614, 0.4269121587276459, 0.395396888256073, 0.4214935898780823, 0.3847975730895996, 0.39301925897598267, 0.3746422827243805, 0.3744593560695648, 0.3986023962497711, 0.4307503402233124, 0.4255581200122833, 0.4080398380756378, 0.40987324714660645, 0.4179075062274933, 0.4424220323562622, 0.4156458079814911, 0.42195332050323486, 0.40444517135620117, 0.40442436933517456, 0.4006117284297943, 0.3936195373535156, 0.4030359089374542, 0.38848042488098145, 0.3923753798007965, 0.3789561986923218, 0.3771015405654907, 0.3851015865802765, 0.3741493821144104, 0.36887305974960327, 0.3821471929550171, 0.37448713183403015, 0.3689042031764984, 0.37663716077804565, 0.3666374981403351, 0.3708885610103607, 0.37771764397621155, 0.38829949498176575, 0.37497925758361816, 0.38493096828460693, 0.3855524957180023, 0.3759198486804962, 0.3965596556663513, 0.39854398369789124, 0.40597352385520935, 0.4093116819858551, 0.4015483260154724, 0.38639402389526367, 0.38334742188453674, 0.3641206622123718, 0.38476353883743286, 0.392596960067749, 0.3830457627773285, 0.37900272011756897, 0.37910890579223633, 0.3805130422115326, 0.36890652775764465, 0.3673223853111267, 0.3753187954425812, 0.37961408495903015, 0.3832332193851471, 0.36954614520072937, 0.37666791677474976, 0.38903146982192993], 'accuracy': [0.39332500100135803, 0.5169249773025513, 0.5690500140190125, 0.5980749726295471, 0.6185500025749207, 0.6446499824523926, 0.6542249917984009, 0.6702749729156494, 0.6834750175476074, 0.6866499781608582, 0.6997249722480774, 0.7064250111579895, 0.7061499953269958, 0.7213749885559082, 0.7261750102043152, 0.730650007724762, 0.7382000088691711, 0.7430750131607056, 0.7493249773979187, 0.7591750025749207, 0.7547500133514404, 0.7554000020027161, 0.76214998960495, 0.7715749740600586, 0.7835749983787537, 0.783174991607666, 0.7884500026702881, 0.7853749990463257, 0.7927250266075134, 0.7947499752044678, 0.8001750111579895, 0.8061249852180481, 0.8113999962806702, 0.811424970626831, 0.8134999871253967, 0.8184999823570251, 0.8199499845504761, 0.8267999887466431, 0.8271750211715698, 0.8332250118255615, 0.832099974155426, 0.845674991607666, 0.8482499718666077, 0.8435750007629395, 0.8516250252723694, 0.8558250069618225, 0.8600749969482422, 0.8559250235557556, 0.8630499839782715, 0.8623999953269958, 0.8714749813079834, 0.8696500062942505, 0.866225004196167, 0.8761000037193298, 0.871399998664856, 0.8831499814987183, 0.8832250237464905, 0.8885499835014343, 0.88919997215271, 0.885200023651123, 0.8848999738693237, 0.8925250172615051, 0.8938249945640564, 0.8998749852180481, 0.8948749899864197, 0.9037250280380249, 0.9018750190734863, 0.9046249985694885, 0.9072250127792358, 0.9048500061035156, 0.9063249826431274, 0.9003999829292297, 0.9005749821662903, 0.9083750247955322, 0.9074249863624573, 0.9071750044822693, 0.9060500264167786, 0.9089000225067139, 0.9151999950408936, 0.9207749962806702, 0.9211000204086304, 0.9283249974250793, 0.9202250242233276, 0.916824996471405, 0.9254249930381775, 0.927049994468689, 0.9223250150680542, 0.9110000133514404, 0.9220749735832214, 0.9234499931335449, 0.9235000014305115, 0.9201250076293945, 0.9204249978065491, 0.9332500100135803, 0.9314500093460083, 0.9304500222206116, 0.9322999715805054, 0.9260249733924866, 0.9275000095367432, 0.9127249717712402, 0.9267749786376953, 0.9324250221252441, 0.9304500222206116, 0.9309499859809875, 0.9287499785423279, 0.9344000220298767, 0.9329249858856201, 0.937250018119812, 0.9396499991416931, 0.9417999982833862, 0.9386500120162964, 0.9268749952316284, 0.9314249753952026, 0.9362499713897705, 0.9321249723434448, 0.9403499960899353, 0.9373499751091003, 0.9349250197410583, 0.9359250068664551, 0.9401749968528748, 0.9440749883651733, 0.9437000155448914, 0.9435250163078308, 0.9459750056266785, 0.9428750276565552, 0.945900022983551, 0.9495499730110168, 0.9362000226974487, 0.9373999834060669, 0.9431999921798706, 0.9355999827384949, 0.9420499801635742, 0.9420999884605408, 0.9406750202178955, 0.9404000043869019, 0.9419000148773193, 0.9419500231742859, 0.941474974155426, 0.9348499774932861, 0.9455249905586243, 0.9375, 0.9509249925613403, 0.946025013923645, 0.9523500204086304, 0.9521999955177307, 0.9447500109672546, 0.9336000084877014, 0.9368249773979187, 0.9433500170707703, 0.9424750208854675, 0.9404749870300293, 0.9328749775886536, 0.9417750239372253, 0.9399250149726868, 0.9455249905586243, 0.9466249942779541, 0.9474999904632568, 0.9488999843597412, 0.9444500207901001, 0.9492999911308289, 0.9490249752998352, 0.9529250264167786, 0.9535999894142151, 0.9493499994277954, 0.9535250067710876, 0.9534249901771545, 0.9504250288009644, 0.9519000053405762, 0.9544500112533569, 0.9522250294685364, 0.9549499750137329, 0.9534500241279602, 0.9506499767303467, 0.9481499791145325, 0.952049970626831, 0.9485999941825867, 0.9494749903678894, 0.9530249834060669, 0.944724977016449, 0.9452999830245972, 0.9433000087738037, 0.9422000050544739, 0.9451749920845032, 0.9504500031471252, 0.9515500068664551, 0.9569000005722046, 0.9497249722480774, 0.9478999972343445, 0.9513750076293945, 0.9523249864578247, 0.953249990940094, 0.9520000219345093, 0.9557999968528748, 0.95517498254776, 0.9535750150680542, 0.9530749917030334, 0.9502999782562256, 0.954200029373169, 0.9527000188827515, 0.9487749934196472], 'val_loss': [2.722346305847168, 3.3317110538482666, 3.775867462158203, 4.10158109664917, 4.118308067321777, 3.4276323318481445, 3.201117515563965, 3.338989734649658, 2.7258987426757812, 2.5052223205566406, 2.043975591659546, 1.9752259254455566, 1.8576546907424927, 1.597412347793579, 1.2791800498962402, 1.1231284141540527, 0.9587628841400146, 1.0323656797409058, 0.9527168869972229, 1.0020078420639038, 0.9451591968536377, 0.9540451169013977, 0.8662941455841064, 0.8589727282524109, 0.9130575060844421, 0.9649269580841064, 0.9425823092460632, 1.1327089071273804, 0.9323857426643372, 0.9256359338760376, 0.9528539776802063, 0.804227352142334, 0.8165379166603088, 1.1415705680847168, 1.0062140226364136, 0.85305255651474, 1.4423068761825562, 0.8207113146781921, 0.8195469975471497, 1.0031088590621948, 0.8809305429458618, 0.8574486374855042, 0.9125652313232422, 0.8173781037330627, 0.7896195650100708, 1.3281404972076416, 0.8986941576004028, 0.9756315350532532, 0.9166879653930664, 0.8054965138435364, 0.825019359588623, 0.8912556767463684, 0.8360372185707092, 0.9389324188232422, 0.8751962184906006, 0.9052926898002625, 0.901474118232727, 0.9235398173332214, 0.9240221977233887, 0.8909043073654175, 0.8815492987632751, 0.8429673910140991, 0.8220181465148926, 0.8261771202087402, 0.8523113131523132, 0.9535934329032898, 0.901904284954071, 0.9043187499046326, 0.990999698638916, 0.9706217050552368, 0.9089002013206482, 0.9417126774787903, 1.07423734664917, 1.0382436513900757, 1.0545932054519653, 0.9496535062789917, 0.9526785016059875, 0.858074426651001, 0.911085844039917, 1.0063482522964478, 0.8790130019187927, 0.909336507320404, 0.880552351474762, 0.92246413230896, 0.8955526351928711, 0.9067089557647705, 0.9901509881019592, 0.9954503774642944, 0.9147337079048157, 1.1071217060089111, 0.9114081263542175, 0.9023227691650391, 0.9028226733207703, 0.9302543997764587, 0.8720269799232483, 1.02116060256958, 0.8960895538330078, 0.9719319343566895, 1.0717780590057373, 0.8863065242767334, 0.947762131690979, 0.993324875831604, 0.8989182710647583, 0.9743923544883728, 0.9599695205688477, 0.9435423612594604, 0.9366759061813354, 0.8574256896972656, 0.865211009979248, 0.9032998085021973, 0.9201050996780396, 0.9325226545333862, 0.9279716610908508, 0.9350337982177734, 0.9352221488952637, 0.9502517580986023, 0.975686252117157, 1.1970326900482178, 0.955520510673523, 0.8974312543869019, 0.890558123588562, 0.9447962641716003, 0.8907617330551147, 0.9652268290519714, 0.9984527230262756, 0.9097933769226074, 1.1073830127716064, 1.037874698638916, 0.9589900374412537, 1.0486112833023071, 0.8919211030006409, 0.9301825165748596, 0.9131590723991394, 1.0174784660339355, 0.9656667113304138, 0.9751700162887573, 0.9420531392097473, 0.9915388822555542, 0.9058839082717896, 0.9261667132377625, 0.9212229251861572, 0.9495443105697632, 0.9828537106513977, 0.9294461011886597, 0.8904146552085876, 0.9053257703781128, 1.0169912576675415, 0.9168246388435364, 0.9434107542037964, 0.9638139605522156, 1.021848440170288, 0.9484723806381226, 1.1103389263153076, 0.9615134000778198, 0.8912755846977234, 0.9677090644836426, 0.9167100787162781, 1.1227093935012817, 0.9151999950408936, 0.9101886749267578, 0.9095084071159363, 0.9899263381958008, 0.969961941242218, 0.9277667999267578, 0.9333702921867371, 0.9150735139846802, 0.9187493324279785, 0.9022094011306763, 0.9447737336158752, 0.9238625764846802, 0.9334216117858887, 0.9594891667366028, 0.9106804728507996, 0.9150502681732178, 0.9293691515922546, 0.9270308613777161, 0.9937407374382019, 0.9373916983604431, 1.0997389554977417, 0.8983379602432251, 0.9426682591438293, 1.0253987312316895, 0.9215105175971985, 0.9465792179107666, 1.0061129331588745, 0.8929389715194702, 1.0103594064712524, 1.1736656427383423, 1.0054574012756348, 1.013250708580017, 0.9134101271629333, 0.9609229564666748, 0.9093874096870422, 0.9967924952507019, 1.001381278038025, 1.0169161558151245, 0.9286288022994995, 0.9403539896011353, 0.9252505898475647, 0.9953796863555908], 'val_accuracy': [0.09520000219345093, 0.09520000219345093, 0.12720000743865967, 0.14249999821186066, 0.131400004029274, 0.11860000342130661, 0.1501999944448471, 0.15060000121593475, 0.23659999668598175, 0.2529999911785126, 0.3034999966621399, 0.4065999984741211, 0.4602000117301941, 0.499099999666214, 0.5889000296592712, 0.6378999948501587, 0.7020000219345093, 0.6802999973297119, 0.7064999938011169, 0.6898999810218811, 0.714900016784668, 0.7182000279426575, 0.7441999912261963, 0.7493000030517578, 0.7275000214576721, 0.7089999914169312, 0.7261999845504761, 0.6607999801635742, 0.7250999808311462, 0.7254999876022339, 0.7235999703407288, 0.7750999927520752, 0.7630000114440918, 0.6599000096321106, 0.7127000093460083, 0.7634000182151794, 0.6118000149726868, 0.76910001039505, 0.7674000263214111, 0.7168999910354614, 0.7491999864578247, 0.7599999904632568, 0.7430999875068665, 0.7721999883651733, 0.784500002861023, 0.638700008392334, 0.7457000017166138, 0.7312999963760376, 0.7473000288009644, 0.7839000225067139, 0.7774999737739563, 0.761900007724762, 0.7785999774932861, 0.745199978351593, 0.76419997215271, 0.7534999847412109, 0.7577000260353088, 0.7476999759674072, 0.7497000098228455, 0.7631999850273132, 0.7703999876976013, 0.7799000144004822, 0.788100004196167, 0.7878000140190125, 0.7825999855995178, 0.7468000054359436, 0.7745000123977661, 0.7702999711036682, 0.7465999722480774, 0.7458999752998352, 0.7717000246047974, 0.7620000243186951, 0.7383000254631042, 0.7389000058174133, 0.7397000193595886, 0.7638999819755554, 0.767799973487854, 0.794700026512146, 0.7807000279426575, 0.7505999803543091, 0.7908999919891357, 0.7767999768257141, 0.7827000021934509, 0.7742999792098999, 0.7857000231742859, 0.7853999733924866, 0.7646999955177307, 0.7641000151634216, 0.7766000032424927, 0.7371000051498413, 0.777999997138977, 0.7885000109672546, 0.7897999882698059, 0.7721999883651733, 0.7957000136375427, 0.7598000168800354, 0.7930999994277954, 0.7621999979019165, 0.7437999844551086, 0.7944999933242798, 0.7847999930381775, 0.767799973487854, 0.794700026512146, 0.7731999754905701, 0.7799000144004822, 0.7872999906539917, 0.7882000207901001, 0.8057000041007996, 0.8012999892234802, 0.7990999817848206, 0.7915999889373779, 0.7807000279426575, 0.7888000011444092, 0.7904999852180481, 0.7907999753952026, 0.7846999764442444, 0.7910000085830688, 0.728600025177002, 0.7827000021934509, 0.7944999933242798, 0.7994999885559082, 0.7839000225067139, 0.798799991607666, 0.7770000100135803, 0.7712000012397766, 0.7953000068664551, 0.7455999851226807, 0.7587000131607056, 0.785099983215332, 0.762499988079071, 0.7943999767303467, 0.7865999937057495, 0.786300003528595, 0.7720000147819519, 0.7882999777793884, 0.7795000076293945, 0.7847999930381775, 0.7752000093460083, 0.7955999970436096, 0.7860999703407288, 0.7950000166893005, 0.7907999753952026, 0.7851999998092651, 0.7926999926567078, 0.7971000075340271, 0.7939000129699707, 0.7760999798774719, 0.7972999811172485, 0.7900999784469604, 0.7885000109672546, 0.7738999724388123, 0.784600019454956, 0.7551000118255615, 0.7850000262260437, 0.8011000156402588, 0.7888000011444092, 0.7865999937057495, 0.754800021648407, 0.7993999719619751, 0.7939000129699707, 0.7929999828338623, 0.777999997138977, 0.7770000100135803, 0.795199990272522, 0.786899983882904, 0.7997999787330627, 0.800599992275238, 0.800599992275238, 0.7954999804496765, 0.7986000180244446, 0.8033999800682068, 0.7942000031471252, 0.7985000014305115, 0.7978000044822693, 0.7901999950408936, 0.795199990272522, 0.7825999855995178, 0.7961000204086304, 0.7573999762535095, 0.7957000136375427, 0.7924000024795532, 0.7791000008583069, 0.8015000224113464, 0.7936999797821045, 0.7807999849319458, 0.8057000041007996, 0.7818999886512756, 0.7466999888420105, 0.7825999855995178, 0.7797999978065491, 0.7946000099182129, 0.7896000146865845, 0.8023999929428101, 0.7867000102996826, 0.7785000205039978, 0.7817000150680542, 0.7955999970436096, 0.7918000221252441, 0.7949000000953674, 0.7797999978065491]}\n",
            "[0.3875266909599304, 0.9588000178337097]\n",
            "[0.8741368055343628, 0.7979999780654907]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}